<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Adaptive Diffusion Policy Project</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f4;
            color: #333;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #0056b3;
            color: #fff;
            padding: 20px 0;
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        section {
            max-width: 900px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        section h2 {
            color: #0056b3;
            margin-bottom: 15px;
        }
        section p {
            margin-bottom: 20px;
        }
        section ul {
            margin-bottom: 20px;
        }
        footer {
            text-align: center;
            background-color: #0056b3;
            color: white;
            padding: 10px 0;
            margin-top: 20px;
        }
        footer a {
            color: #fff;
            text-decoration: underline;
        }
        img {
            width: 100%;  /* Set image width to 100% of the section */
            max-width: 900px;  /* Max width set to 900px, same as the section width */
            height: auto;  /* Maintain the aspect ratio */
            display: block;
            margin: 10px 0;  /* Add vertical margin for space */
        }
    </style>
</head>
<body>

<header>
    <h1>Adaptive Diffusion Policy (ADP) for Legged Locomotion</h1>
</header>

<section>
    <h2>Abstract</h2>
    <p>The development of efficient and robust locomotion policies for legged robots remains a significant challenge, often necessitating intricate reward function design and extensive dataset typically in conventional offline reinforcement learning frameworks. To solve this problem, we introduce Adaptive Diffusion Policy (ADP), a novel approach by employing a diffusion policy that generates robot action sequences from observations via offline training. Unlike conventional online reinforcement learning, which requires hundreds of million environment interactions, the offline setting of ADP allows for more sample-efficient learning by leveraging one million pre-collected data. By leveraging the stochastic nature of diffusion models, ADP is able to capture the underlying dynamics of diverse terrains, enabling more robust decision-making compared to deterministic RL policies. To further enhance the performance of the ADP, we employed a reward optimization fine-tuning phase. This fine-tuning process allowed for the refinement of the policy, enabling the robot to achieve even better adaptation and higher rewards across various terrains. This work contributes to the field by offering a promising alternative to purely online RL learning, potentially advancing the development of more efficient and adaptable locomotion strategies for legged robots. Experimental results demonstrate that ADP enables the robot to adaptively navigate diverse terrains, while achieving superior rewards and improving utilization efficiency compared to conventional RL baselines. Upon the publication of this paper, the code will be open source.</p>
</section>

<section>
    <h2>Gap</h2>

    <video width="900" controls>
        <source src="images/Rec.mp4" type="video/mp4">
    </video>

</section>

<section>
    <h2>Methods</h2>
    
    <!-- First Image -->
    <img src="images/Structure.png" alt="ADP Training Framework">
    
    <!-- Second Image -->
    <img src="images/rollout.png" alt="Robot control using Adaptive Diffusion Policy">
</section>



<footer>
    <p>&copy; 2024 ADP Project. All Rights Reserved.</p>
</footer>

</body>
</html>
